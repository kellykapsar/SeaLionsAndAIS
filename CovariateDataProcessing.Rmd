---
title: "Covariate Data Processing"
author: "Kelly Kapsar"
date: "11/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import libraries. 
```{r message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(sf)
library(raster)
library(ggplot2)
library(marmap)
library(ncdf4)
library(anytime)
library(rgdal)

```

Study area boundaries. 

```{r warning=FALSE}
# Location of study area shape file on my computer
study <- read_sf("../Data_Raw/DraftSeaLionAOI_Envelope_20kmBuff_6334.shp")
basemap <- read_sf("../Data_Raw/BBmap.shp") %>% st_transform(6334) %>% st_buffer(0)

# Projection information for NAT83/UTM Zone 5N (EPSG:6334)
prj <- "+proj=utm +zone=5 +ellps=GRS80 +units=m +no_defs"

# Crop basemap to buffered extent of study area 
study.buff <- st_buffer(study, 100000) # Buffer study area by 100 km
basemap.crop <- st_crop(basemap, study.buff)

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")

# Get latlong coordinates for study area for use in downloading other data sets
studylatlon <- study %>% st_transform(4269) %>% st_bbox()
```

1 km land mask layer. 

```{r land mask}
# Open netcdf file 
landmask <- nc_open("../Data_Raw/arc/base/analysis_masks/kod_mask_1k_u5w84.nc")
# Save metadata to a text file
{
  sink('../Data_Raw/arc/base/analysis_masks/kod_mask_1k_u5w84.txt')
  print(landmask)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(landmask, "x")
lat <- ncvar_get(landmask, "y", verbose = F)

head(lon)

# Read in data from the variable and verify the dimensions of the array
landmask.array <- ncvar_get(landmask, "kod_mask_1k_u5w84") # 3dim array
dim(landmask.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(landmask, "kod_mask_1k_u5w84","missing_value")
fillvalue

landmask.array[landmask.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(landmask)

# Create raster of landmask
landmask.r <- raster(t(landmask.array), 
                xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # CRS from metadata
                crs=CRS("+proj=utm +zone=5 +datum=WGS84 +units=m +no_defs")) 

landmask.df <- as.data.frame(landmask.r, xy=TRUE)
colnames(landmask.df) <- c("x","y","water")

plot(landmask.r)

# Map of study area with landmask data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=landmask.df, aes(x=x, y=y, fill=water), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")
```

## Sea Lion location geodatabase

```{r sea lion gdb processing}
# Import sea lion location geodatabase (downloaded from Google Drive folder)
seali <- st_layers("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb")
seali$name # list of layers 

# Determine all layers of interest within gdb 
lyrs <- grep("_loc", seali$name)

# Create initial sf object with data from one sea lion
sealis <- st_read("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb",
                    layer=seali$name[lyrs[1]])

# Remove that layer from the layers of interest list
lyrs <- lyrs[2:length(lyrs)]

# Append all other layers of interest onto the main location data set 
for(i in 1:length(lyrs)){
  temp <- st_read("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb",
                    layer=seali$name[lyrs[i]])
  colnames(temp) <- colnames(sealis)
  sealis <- rbind(sealis, temp)
}

# Remove points outside the study area
sealis$inbounds <- lengths(st_within(sealis, st_transform(study, 4326)))
sealis <- sealis[which(sealis$inbounds == TRUE),]

############ CHECK ON THIS ###############
# 40 Duplicated rows (excluding geometry column)
test <- duplicated(data.frame(sealis))
# Remove duplicated rows 
sealis <- sealis[which(duplicated(data.frame(sealis))==FALSE),]

# Transform geometry to correct projection 
st_geometry(sealis) <- st_transform(st_geometry(sealis),prj)

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")+
  geom_sf(data=sealis, aes(color=DeployID))

``` 

## AIS Data
```{r ais, warning=FALSE}

# Import raster, reproject to 4336, and crop to study area
ships <- raster::raster("../Data_Processed/AIS/AISRasterSubset_2018-11-Cargo_5000m.tif", crs="+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") 

shipsDf <- as.data.frame(ships, xy=TRUE) 
colnames(shipsDf) <- c("x","y","km")

# Save raster object 
# raster::writeRaster(bathy, "../Data_Processed/bathemtry_GEBCO_cropped_4336.tif")

# Map of study area with bathymetric data 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=shipsDf, aes(x=x, y=y, fill=km), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

```

## Bathymetry 

Bathymetry data were collected from the [General Bathymetric Chart of the Oceans](gebco.net). You can use the marmap package to load the data and create a "bathy" object type, but I'm not sure exactly how to work with that kind of object, so I just imported the tif data and converted into a raster.    

```{r bathymetry, warning=FALSE}

# bathymarmap <- marmap::readGEBCO.bathy("../Data_Raw/GEBCO_2020_09_Nov_2020_nc/gebco_2020_n61.0_s55.0_w-155.0_e-147.0.nc")

# Import raster, reproject to 4336, and crop to study area
bathy <- raster::raster("../Data_Raw/GEBCO_2020_09_Nov_2020_tif/gebco_2020_n61.0_s55.0_w-155.0_e-147.0.tif") %>% projectRaster(crs=prj) %>% raster::crop(study)

bathyDf <- as.data.frame(bathy, xy=TRUE) 
colnames(bathyDf) <- c("x","y","depth")

# Save raster object 
# raster::writeRaster(bathy, "../Data_Processed/bathemtry_GEBCO_cropped_4336.tif")

# Map of study area with bathymetric data 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=bathyDf, aes(x=x, y=y, fill=depth), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

```


```{r slope}

slope <- terrain(bathy, opt="slope", unit="radians", neighbors=8)


slopeDf <- as.data.frame(slope, xy=TRUE) 
colnames(slopeDf) <- c("x","y","slope")

# Save raster object 
# raster::writeRaster(bathy, "../Data_Processed/bathemtry_GEBCO_cropped_4336.tif")

# Map of study area with bathymetric data 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=slopeDf, aes(x=x, y=y, fill=slope), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

```

Calculate distance to land 

```{r dist2isobath}
# ESTIMATED COORDINATES --- WILL NEED TO UPDATE
# Get depth data from NOAA 
depth <- getNOAA.bathy(lat1=55, lat2=61, lon1=-146, lon2=-156, resolution = 1)

# Creating color palettes
blues <- c("lightsteelblue4", "lightsteelblue3",
"lightsteelblue2", "lightsteelblue1")
greys <- c(grey(0.6), grey(0.93), grey(0.99))

# Plot depth data 
plot(depth, image = TRUE, land = TRUE, lwd = 0.03, 
     bpal = list(c(0, max(depth), greys),c(min(depth), 0, blues)))
# Add coastline
plot(depth, n = 1, lwd = 0.4, add = TRUE)

# summary(depth)

# Transect of depth profile at 58N across study area 
trsect <- get.transect(depth, -155, 58, -148, 58, distance = TRUE)
plotProfile(trsect)


# Calculate distance to nearest NA cell for all cells in landmask raster 
distland2 <- bathy
values(distland2)[values(distland2) <= 0] = NA
distland2 <- raster::distance(distland2)

distland2.df <- as.data.frame(distland2, xy=TRUE)
colnames(distland2.df) <- c("x","y","distland")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=distland2.df, aes(x=x, y=y, fill=distland), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")
```

Calculate distance to nearest cell between 400 and 500 m depth. 

```{r 500m shelf break}
dist500m <- bathy
values(dist500m)[values(dist500m) >= -400 | values(dist500m) <= -500] = NA
dist500m <- raster::distance(dist500m)

dist500m.df <- as.data.frame(dist500m, xy=TRUE)
colnames(dist500m.df) <- c("x","y","dist500m")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=dist500m.df, aes(x=x, y=y, fill=dist500m), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

```

## Wind Speed 

Wind speed data from the Copernicus Marine Service (Met-Op A and B satellites).

Data set name: [GLOBAL OCEAN WIND L4 NEAR REAL TIME 6 HOURLY OBSERVATIONS](https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=WIND_GLO_WIND_L4_NRT_OBSERVATIONS_012_004). 

```{r wind speed}

# Open netcdf file 
wind <- nc_open("../Data_Raw/CERSAT-GLO-BLENDED_WIND_L4-V6-OBS_FULL_TIME_SERIE_1612900944654.nc")
# Save metadata to a text file
{
  sink('../Data_Raw/CERSAT-GLO-BLENDED_WIND_L4-V6-OBS_FULL_TIME_SERIE_1612900944654.txt')
  print(wind)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(wind, "lon")
lat <- ncvar_get(wind, "lat", verbose = F)
t <- ncvar_get(wind, "time")

head(lon)

# Read in data from the wind variable and verify the dimensions of the array
wind.array <- ncvar_get(wind, "wind_speed") # 3dim array
dim(wind.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(wind, "wind_speed","_FillValue")
fillvalue

wind.array[wind.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(wind)

# Isolate and plot a random time step to check
wind.slice <- wind.array[,,1]

dim(wind.slice) #2dim

wind.r <- raster(t(wind.slice), xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # Found projection on the website
                crs=CRS("+proj=longlat +datum=WGS84 +no_defs")) %>%
         flip(direction="y")%>%
         raster::projectRaster(crs=prj)

wind.df <- as.data.frame(wind.r, xy=TRUE)
colnames(wind.df) <- c("x","y","windspeed")

plot(wind.r)


# Map of study area with wind data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=wind.df, aes(x=x, y=y, fill=windspeed), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

# Make a raster brick of all values 
wind_brick <- brick(wind.array, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS("+proj=longlat +datum=WGS84 +no_defs")) %>% 
         flip(direction="x") %>%
         raster::projectRaster(crs=prj) %>% 
         raster::crop(study)

# Convert date from seconds since 01/01/1970 to yyyy-mm-dd format
t2 <- as.POSIXct("1900-01-01 00:00")+as.difftime(t,units="hours")
class(t2)
# Name raster layers after the date that they portray
names(wind_brick) <-t2

# Save output file
# writeRaster(wind_brick, "../Data_Processed/wind_AOOS_cropped_4336.tif")

# Fun animation of the raster brick
# animate(wind_brick, pause=0.5, n=1)

# Calculate mean monthly wind rasters 

#get the date from the names of the layers and extract the month
indices <- format(as.Date(names(wind_brick), format = "X%Y.%m.%d"), format = "%Y%m")
indices <- as.numeric(indices)

#sum layers
wind_month <- stackApply(wind_brick, indices, fun = mean)
# animate(wind_month, pause=0.5, n=1)

# plot monthly average wind data for November, 2018
windmonth.df <- as.data.frame(wind_month[[1]], xy=TRUE)
colnames(windmonth.df) <- c("x","y","windspeed")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=windmonth.df, aes(x=x, y=y, fill=windspeed), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")


# Save output file
# writeRaster(wind_month, "../Data_Processed/WIND_Monthly_cropped_4336.nc", 
#               overwrite=TRUE, format="CDF",     
#               varname="wind_speed", varunit="m/s", 
#               longname="Wind Speed -- raster brick to netCDF", 
#               xname="lon",   yname="lat",zname="time",
#               zunit="numeric")
```

Test to make sure that the netcdf file saved correctly. 

```{r testing netcdf file save}

test <-nc_open("../Data_Processed/WIND_Monthly_cropped_4336.nc")

# Save metadata to a text file
{
  sink('../Data_Processed/WIND_Monthly_cropped_4336.txt')
  print(test)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(test, "lon")
lat <- ncvar_get(test, "lat", verbose = F)
t <- ncvar_get(test, "time")

head(lon)

# Read in data from the wind variable and verify the dimensions of the array
test.array <- ncvar_get(test, "wind_speed") # 3dim array
dim(test.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(test, "wind_speed","_FillValue")
fillvalue

test.array[test.array == fillvalue$value] <- NA

nc_close(test)

test_brick <- brick(test.array, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS(prj)) 

plot(test_brick[[1]])


test.df <- as.data.frame(test_brick[[1]], xy=TRUE)
colnames(test.df) <- c("x","y","windspeed")

plot(test.r)


# Map of study area with wind data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=test.df, aes(x=x, y=y, fill=windspeed), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")


```

## Sea Surface Temperature

[OSTIA: Operational Sea Surface Temperature and Sea Ice Analysis](https://portal.aoos.org/gulf-of-alaska.php#module-metadata/e873784f-7201-416f-bb26-6061c82af388/adc56dd8-e6c6-4d09-abc6-86962fc308eb) from the Alaska Ocean Observing System. 

Citation: 	UK Met Office. 2005. OSTIA L4 SST Analysis. Ver. 1.0. PO.DAAC, CA, USA. Dataset accessed [YYYY-MM-DD] at https://doi.org/10.5067/GHOST-4FK01

Data requested from: https://thredds.axiomdatascience.com/thredds/ncss/AOOS_OSTIA.nc/dataset.html
Variables = analysed_sst, analysis_error, mask, sea_ice_fraction

North = 61
South = 55
West = -156
East = -147
Start = 01 Nov 2018
End = 31 May 2020

NCSS Request URL: https://thredds.axiomdatascience.com/thredds/ncss/AOOS_OSTIA.nc?var=analysed_sst&var=analysis_error&var=mask&var=sea_ice_fraction&north=61&west=-156&east=-147&south=55&horizStride=1&time_start=2018-11-01T12%3A00%3A00Z&time_end=2020-05-31T12%3A00%3A00Z&timeStride=1&accept=netcdf

```{r sea surface temperature}

# Open netcdf file 
sst <- nc_open("../Data_Raw/AOOS_OSTIA.nc")
# Save metadata to a text file
{
  sink('../Data_Raw/AOOS_OSTIA.txt')
  print(sst)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(sst, "lon")
lat <- ncvar_get(sst, "lat", verbose = F)
t <- ncvar_get(sst, "time")

head(lon)

# Read in data from the SST variable and verify the dimensions of the array
sst.array <- ncvar_get(sst, "analysed_sst") # 3dim array
dim(sst.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(sst, "analysed_sst","_FillValue")
fillvalue

sst.array[sst.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(sst)

# Isolate and plot a random time step to check
sst.slice <- sst.array[,,1]

dim(sst.slice) #2dim

sst.r <- raster(t(sst.slice), xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # Found projection on the website
                crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs ")) %>%
         flip(direction="y") %>%
         raster::projectRaster(crs=prj) %>%
         raster::crop(study)

sst.df <- as.data.frame(sst.r, xy=TRUE)
colnames(sst.df) <- c("x","y","sst")

plot(sst.r)

# Map of study area with sst data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=sst.df, aes(x=x, y=y, fill=sst), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

# Make a raster brick of all values 
sst_brick <- brick(sst.array, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")) %>% 
         flip(direction="x") %>%
         raster::projectRaster(crs=prj) %>% 
         raster::crop(study)

# Convert date from seconds since 01/01/1970 to yyyy-mm-dd format
t <- anydate(t)
class(t)
# Name raster layers after the date that they portray
names(sst_brick) <- format(t, "X%Y.%m.%d")

# Save output file
# writeRaster(sst_brick, "../Data_Processed/SST_AOOS_cropped_4336.tif")

# Fun animation of the raster brick
# animate(sst_brick, pause=0.5, n=1)

# Calculate the change between any two layers
# tempchange <- sst_brick[[2]]-sst_brick[[1]]
# tempchange

# Calculate mean monthly SST rasters 

#get the date from the names of the layers and extract the month
indices <- format(as.Date(names(sst_brick), format = "X%Y.%m.%d"), format = "%Y%m")
indices <- as.numeric(indices)

#sum layers
sst_month <- stackApply(sst_brick, indices, fun = mean)
# animate(sst_month, pause=0.5, n=1)

```
I used this tutorial on indexing [raster time series](https://cyberhelp.sesync.org/raster-time-series-alaska-lesson/index.html#/slides/brick)

For calculating monthly mean values in a [raster brick](https://gis.stackexchange.com/questions/237272/mean-by-month-on-r-stacked-raster). 

```{r merging sea lion locs with SST data, warnings=F, messages=F}

# Put seali date into same format as raster brick names
sealis$yearmon <- format(sealis$Date_, format="%Y%m")

# Extract monghtly average values for SST at seali locs
# Want to look into using an apply function to hopefullymake this go faster (it takes ~5 min right now)
for(i in 1:length(sealis$DeployID)){
  if(i %% 100 == 0){
    print(i)
  }
   sealis$SST[i] <- extract(subset(sst_month, 
                                   paste0("index_",sealis$yearmon[i])), sealis[i,])
}

# Looking at the data 
hist(sealis$SST)
length(which(is.na(sealis$SST)))
length(unique(sealis$SST))
length(sealis$SST)
tail(sealis$SST)

hist(as.numeric(sealis$yearmon))
unique(sealis$yearmon)
unique(sealis$DeployID)

# # SST over time at a particular location 
# idx <- match('Aug.13.2019', names(sst_brick))
# plot(sst_brick[[idx]])

```


```{r extra fun stuff}
# Correlation between dive temperature and ste
sealitemps <- st_read("../Data_Raw/locs_depths_temps_geodbase_20201130.gdb",
                    layer=seali$name[10]) %>% st_transform(prj)

# Extract monghtly average values for SST at seali locs
# Want to look into using an apply function to hopefullymake this go faster (it takes ~5 min right now)
for(i in 1:length(sealitemps$DeployID)){
  if(i %% 100 == 0){
    print(i)
  }
   sealitemps$SST[i] <- extract(subset(sst_month, 
                                   paste0("index_",sealitemps$yearmon[i])), sealis[i,])
}


```

