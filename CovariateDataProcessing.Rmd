---
title: "Covariate Data Processing"
author: "Kelly Kapsar"
date: "11/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import libraries. 
```{r message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(sf)
library(raster)
library(ggplot2)
library(marmap)
library(ncdf4)
library(anytime)
library(rgdal)
library(scales)

```

Study area boundaries. 

```{r study area warning=FALSE}
# Projection information for WGS84/UTM Zone 5N (EPSG:32605)
prj <- 32605

# Create study area polygon
coords <- data.frame(lat=c(56, 62, 62, 56, 56), lon=c(-155, -155, -143, -143, -155), id="study")
study <- coords %>% 
         st_as_sf(coords = c("lon", "lat"), crs=4326) %>% 
         group_by(id) %>% 
         summarize(geometry = st_combine(geometry)) %>%  
         st_cast("POLYGON") %>% 
         st_transform(prj)
# st_write(study, "../Data_Raw/studyarea.shp")

basemap <- read_sf("../Data_Raw/BBmap.shp") %>% st_transform(prj) %>%  st_buffer(0)

# Crop basemap to buffered extent of study area 
study.buff <- st_buffer(study, 100000) # Buffer study area by 100 km
basemap.crop <- st_crop(basemap, study.buff)

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")

# Get latlong coordinates for study area for use in downloading other data sets
studylatlon <- study %>% st_transform(4269) %>% st_bbox()
```

1 km land mask layer. 

```{r land mask kodiak}
# Open netcdf file 
landmask <- nc_open("../Data_Raw/arc/base/analysis_masks/kod_mask_1k_u5w84.nc")
# Save metadata to a text file
{
  sink('../Data_Raw/arc/base/analysis_masks/kod_mask_1k_u5w84.txt')
  print(landmask)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(landmask, "x")
lat <- ncvar_get(landmask, "y", verbose = F)

head(lon)

# Read in data from the variable and verify the dimensions of the array
landmask.array <- ncvar_get(landmask, "kod_mask_1k_u5w84") # 3dim array
dim(landmask.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(landmask, "kod_mask_1k_u5w84","missing_value")
fillvalue

landmask.array[landmask.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(landmask)

# Create raster of landmask
landmask.r <- raster(t(landmask.array), 
                xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # CRS from metadata
                crs=CRS("+proj=utm +zone=5 +datum=WGS84 +units=m +no_defs")) 

landmask.df <- as.data.frame(landmask.r, xy=TRUE)
colnames(landmask.df) <- c("x","y","water")

plot(landmask.r)

# Map of study area with landmask data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=landmask.df, aes(x=x, y=y, fill=water), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")
```

```{r land mask pws}

############################################################
# SOMETHING'S WRONG WITH THE PROJECTION...IT'S APPEARING IN ZONE 5 INSTEAD OF ZONE 6

# Open netcdf file 
landmask2 <- nc_open("../Data_Raw/arc/base/analysis_masks/pws_mask_1k_u6w84.nc")
# Save metadata to a text file
{
  sink('../Data_Raw/arc/base/analysis_masks/pws_mask_1k_u6w84.txt')
  print(landmask2)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(landmask2, "x")
lat <- ncvar_get(landmask2, "y", verbose = F)

head(lon)

# Read in data from the variable and verify the dimensions of the array
landmask2.array <- ncvar_get(landmask2, "pws_mask_1k_u6w84") # 3dim array
dim(landmask2.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(landmask2, "pws_mask_1k_u6w84","missing_value")
fillvalue

landmask2.array[landmask2.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(landmask2)

# Create raster of landmask
landmask2.r <- raster(t(landmask2.array), 
                xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # CRS from metadata
                crs=CRS("+proj=utm +zone=6 +datum=WGS84 +units=m +no_defs")) 

landmask2.df <- as.data.frame(landmask2.r, xy=TRUE)
colnames(landmask2.df) <- c("x","y","water")

plot(landmask2.r)

# Map of study area with landmask data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=landmask2.df, aes(x=x, y=y, fill=water), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")
```

## Sea Lion location geodatabase

```{r sea lion gdb processing}
# Import sea lion location geodatabase (downloaded from Google Drive folder)
seali <- st_layers("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb")
seali$name # list of layers 

# Determine all layers of interest within gdb 
lyrs <- grep("_loc", seali$name)

# Create initial sf object with data from one sea lion
sealis <- st_read("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb",
                    layer=seali$name[lyrs[1]])

# Remove that layer from the layers of interest list
lyrs <- lyrs[2:length(lyrs)]

# Append all other layers of interest onto the main location data set 
for(i in 1:length(lyrs)){
  temp <- st_read("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb",
                    layer=seali$name[lyrs[i]])
  colnames(temp) <- colnames(sealis)
  sealis <- rbind(sealis, temp)
}

# Remove low quality points 
sealis <- sealis[-which(sealis$Quality %in% c("A","B","0")),]
# sealis$inbounds <- lengths(st_within(sealis, st_transform(study, 4326)))
# sealis <- sealis[which(sealis$inbounds == TRUE),]

############ CHECK ON THIS ###############
# 40 Duplicated rows (excluding geometry column)
test <- duplicated(data.frame(sealis))
# Remove duplicated rows 
sealis <- sealis[which(duplicated(data.frame(sealis))==FALSE),]

# Transform geometry to correct projection 
st_geometry(sealis) <- st_transform(st_geometry(sealis), prj)

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")+
  geom_sf(data=sealis, aes(color=DeployID))

# Tag duration timeline 
timeline <- sealis %>% st_drop_geometry() %>% group_by(DeployID) %>% summarize(starttag = as.Date(min(Date)), endtag=as.Date(max(Date)))

ggplot(timeline, aes(x=starttag, y= DeployID)) +
  geom_linerange(aes(xmin = starttag, xmax = endtag),color = "black",size = 2) + 
  scale_x_date(breaks=date_breaks(width="1 month"), date_labels="%b %Y") +
  theme(axis.text.x=element_text(angle=45, hjust=1))+
  ylab("") +
  xlab("") 


``` 

## AIS Data
```{r ais, warning=FALSE}

# Import raster, reproject to 4336, and crop to study area
ships <- raster::raster("../Data_Processed/AIS/AISRasterSubset_2018-11-Cargo_1000m.tif", crs="+proj=aea +lat_1=55 +lat_2=65 +lat_0=50 +lon_0=-154 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs") %>%  projectRaster(crs=prj)

shipsDf <- as.data.frame(ships, xy=TRUE) %>% drop_na()
colnames(shipsDf) <- c("x","y","km")

# Save raster object 
# raster::writeRaster(bathy, "../Data_Processed/bathemtry_GEBCO_cropped_4336.tif")

# Map of study area with bathymetric data 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=shipsDf, aes(x=x, y=y, fill=km), alpha=0.9) +
  scale_fill_gradient2(trans="log10", low = "black", mid="gray", high="red") +
  geom_sf(data=study, fill=NA, color="red")

```

## Bathymetry 

Bathymetry data were collected from the [General Bathymetric Chart of the Oceans](gebco.net). You can use the marmap package to load the data and create a "bathy" object type, but I'm not sure exactly how to work with that kind of object, so I just imported the tif data and converted into a raster.    

```{r bathymetry, warning=FALSE}

# bathymarmap <- marmap::readGEBCO.bathy("../Data_Raw/GEBCO_2020_09_Nov_2020_nc/gebco_2020_n61.0_s55.0_w-155.0_e-147.0.nc")

# Import raster, reproject to 4336, and crop to study area
bathy <- raster::raster("../Data_Raw/GEBCO_2020_21_Jul_2021/gebco_2020_n62.0_s56.0_w-155.0_e-143.0.tif") %>% projectRaster(crs=prj) %>% raster::crop(study)

bathyDf <- as.data.frame(bathy, xy=TRUE) %>% drop_na()
colnames(bathyDf) <- c("x","y","depth")

# Save raster object 
# raster::writeRaster(bathy, "../Data_Processed/bathemtry_GEBCO_cropped_4336.tif")

# Map of study area with bathymetric data 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=bathyDf, aes(x=x, y=y, fill=depth), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

# writeRaster(bathy, "../Data_Processed/Bathymetry.tif")

```

## Slope

```{r slope}

slope <- terrain(bathy, opt="slope", unit="radians", neighbors=8)


slopeDf <- as.data.frame(slope, xy=TRUE)  %>% drop_na()
colnames(slopeDf) <- c("x","y","slope")

# Save raster object 
# raster::writeRaster(bathy, "../Data_Processed/bathemtry_GEBCO_cropped_4336.tif")

# Map of study area with bathymetric data 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=slopeDf, aes(x=x, y=y, fill=slope), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")


# writeRaster(slope, "../Data_Processed/Slope.tif")
```

## Distance to land 

```{r dist2land}
# # Alternate method: Get depth data from NOAA 
# depth <- getNOAA.bathy(lat1=55, lat2=62, lon1=-143, lon2=-156, resolution = 1)
# 
# # Creating color palettes
# blues <- c("lightsteelblue4", "lightsteelblue3",
# "lightsteelblue2", "lightsteelblue1")
# greys <- c(grey(0.6), grey(0.93), grey(0.99))
# 
# # Plot depth data 
# plot(depth, image = TRUE, land = TRUE, lwd = 0.03, 
#      bpal = list(c(0, max(depth), greys),c(min(depth), 0, blues)))
# # Add coastline
# plot(depth, n = 1, lwd = 0.4, add = TRUE)
# 
# # summary(depth)
# 
# # Transect of depth profile at 58N across study area 
# trsect <- get.transect(depth, -155, 58, -148, 58, distance = TRUE)
# plotProfile(trsect)


# Calculate distance to nearest NA cell for all cells in landmask raster 
distland2 <- bathy
values(distland2)[values(distland2) <= 0] = NA
distland2 <- raster::distance(distland2)

distland2.df <- as.data.frame(distland2, xy=TRUE) %>% drop_na()
colnames(distland2.df) <- c("x","y","distland")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=distland2.df, aes(x=x, y=y, fill=distland), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

# writeRaster(distland2, "../Data_Processed/DistLand.tif")
```

## Distance to shelf break

Calculate distance to nearest cell between 400 and 500 m depth. 

```{r 500m shelf break}
dist500m <- bathy
values(dist500m)[values(dist500m) >= -400 | values(dist500m) <= -500] = NA
dist500m <- raster::distance(dist500m)

dist500m.df <- as.data.frame(dist500m, xy=TRUE)  %>% drop_na()
colnames(dist500m.df) <- c("x","y","dist500m")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=dist500m.df, aes(x=x, y=y, fill=dist500m), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

writeRaster(dist500m, "../Data_Processed/Dist500m.tif")

```

## Sea Surface Height

FORMER Source: [GLOBAL OCEAN 1/4Â° PHYSICS ANALYSIS AND FORECAST UPDATED DAILY](https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=GLOBAL_ANALYSISFORECAST_PHY_CPL_001_015)

NEW Source: [SEALEVEL_GLO_PHY_L4_NRT_OBSERVATIONS_008_046](https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=SEALEVEL_GLO_PHY_L4_NRT_OBSERVATIONS_008_046)

```{r sea surface height - reprocessed}

ssh <- nc_open("../Data_Raw/dataset-duacs-rep-global-merged-allsat-phy-l4_20181101-20200531.nc")

{
  sink('../Data_Raw/dataset-duacs-rep-global-merged-allsat-phy-l4_20181101-20200531.txt')
  print(ssh)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(ssh, "longitude")
lat <- ncvar_get(ssh, "latitude", verbose = F)
t <- ncvar_get(ssh, "time")

head(lon)

# Read in data from the ssh variable and verify the dimensions of the array
ssh.array <- ncvar_get(ssh, "adt") # 3dim array
veln.array <- ncvar_get(ssh, "vgos") 
vele.array <- ncvar_get(ssh, "ugos") 
dim(ssh.array)

#### NO FILL VALUE IN THESE DATA? 
# # Identify fill value and replace with NA
# fillvalue <- ncatt_get(ssh, "adt","_FillValue")
# fillvalue
# 
# ssh.array[ssh.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(ssh)

# Convert longitude from 0-360 to -180 to 180
lon <- lon-360

# Isolate and plot a random time step to check
ssh.slice <- ssh.array[,,1]

dim(ssh.slice) #2dim

ssh.r <- raster(t(ssh.slice), xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # Found projection on the website
                crs=4326) %>%
         flip(direction="y")%>%
         raster::projectRaster(crs=prj) %>%
         raster::crop(study)

ssh.df <- as.data.frame(ssh.r, xy=TRUE) %>% drop_na()
colnames(ssh.df) <- c("x","y","ssh")

plot(ssh.r)


# Map of study area with ssh data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=ssh.df, aes(x=x, y=y, fill=ssh), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")


test <- aperm(ssh.array, c(2,1,3))
# Make a raster brick of all values 
ssh_brick <- brick(test, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS("+proj=longlat +datum=WGS84 +no_defs")) %>% 
         flip(direction="y") %>%
         raster::projectRaster(crs=prj) %>% 
         raster::crop(study)

# Convert date from seconds since 01/01/2016 to yyyy-mm-dd format
# Found this date in metadata text file
t2 <- as.POSIXct("1950-01-01 00:00")+as.difftime(t,units="days")

class(t2)
# Name raster layers after the date that they portray
names(ssh_brick) <-t2

# Save output file
# writeRaster(ssh_brick, "../Data_Processed/ssh.tif")

# Fun animation of the raster brick
# animate(ssh_brick, pause=0.5, n=1)

# Calculate mean monthly ssh rasters 

#get the date from the names of the layers and extract the month
indices <- format(as.Date(names(ssh_brick), format = "X%Y.%m.%d"), format = "%Y%m")
indices <- as.numeric(indices)

#sum layers
ssh_month <- stackApply(ssh_brick, indices, fun = mean)
# animate(ssh_month, pause=0.5, n=1)

# plot monthly average ssh data for November, 2018
sshmonth.df <- as.data.frame(ssh_month[[1]], xy=TRUE)  %>% drop_na()
colnames(sshmonth.df) <- c("x","y","ssh")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=sshmonth.df, aes(x=x, y=y, fill=ssh), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

# animate(ssh_month, pause=0.5, n=1)

# Save output file
# writeRaster(ssh_month, "../Data_Processed/SSH_Monthly_20181101-20200531.nc",
#               overwrite=TRUE, format="CDF",
#               varname="ssh", varunit="m",
#               longname="Sea Surface Height -- raster brick to netCDF",
#               xname="lon",   yname="lat",zname="time",
#               zunit="numeric")


```


```{r sea surface height - nrt}

ssh2 <- nc_open("../Data_Raw/dataset-duacs-nrt-global-merged-allsat-phy-l4_20200601-20200731.nc")

{
  sink('../Data_Raw/dataset-duacs-nrt-global-merged-allsat-phy-l4_20200601-20200731.txt')
  print(ssh2)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(ssh2, "longitude")
lat <- ncvar_get(ssh2, "latitude", verbose = F)
t <- ncvar_get(ssh2, "time")

head(lon)

# Read in data from the ssh2 variable and verify the dimensions of the array
ssh2.array <- ncvar_get(ssh2, "adt") # 3dim array
dim(ssh2.array)

#### NO FILL VALUE IN THESE DATA? 
# # Identify fill value and replace with NA
# fillvalue <- ncatt_get(ssh2, "adt","_FillValue")
# fillvalue
# 
# ssh2.array[ssh2.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(ssh2)

# Convert longitude from 0-360 to -180 to 180
lon <- lon-360

test <- aperm(ssh2.array, c(2,1,3))
# Make a raster brick of all values 
ssh2_brick <- brick(test, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS("+proj=longlat +datum=WGS84 +no_defs")) %>% 
         flip(direction="y") %>%
         raster::projectRaster(crs=prj) %>% 
         raster::crop(study)

# Convert date from seconds since 01/01/2016 to yyyy-mm-dd format
# Found this date in metadata text file
t2 <- as.POSIXct("1950-01-01 00:00")+as.difftime(t,units="days")

class(t2)
# Name raster layers after the date that they portray
names(ssh2_brick) <-t2

# Save output file
# writeRaster(ssh2_brick, "../Data_Processed/ssh2.tif")

# Fun animation of the raster brick
# animate(ssh2_brick, pause=0.5, n=1)

# Calculate mean monthly ssh2 rasters 

#get the date from the names of the layers and extract the month
indices <- format(as.Date(names(ssh2_brick), format = "X%Y.%m.%d"), format = "%Y%m")
indices <- as.numeric(indices)

#sum layers
ssh2_month <- stackApply(ssh2_brick, indices, fun = mean)
# animate(ssh2_month, pause=0.5, n=1)

# plot monthly average ssh data for November, 2018
ssh2month.df <- as.data.frame(ssh2_month[[1]], xy=TRUE)  %>% drop_na()
colnames(ssh2month.df) <- c("x","y","ssh")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=ssh2month.df, aes(x=x, y=y, fill=ssh), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")


temp <- addLayer(ssh_month, ssh2_month)
# animate(ssh_month, pause=0.5, n=1)

# Save output file
# writeRaster(ssh2_month, "../Data_Processed/SSH_Monthly_20200601-20200731.nc",
#               overwrite=TRUE, format="CDF",
#               varname="ssh", varunit="m",
#               longname="Sea Surface Height -- raster brick to netCDF",
#               xname="lon",   yname="lat",zname="time",
#               zunit="numeric")
```

## Wind Speed 

Wind speed data from the Copernicus Marine Service (Met-Op A and B satellites).

Data set name: [GLOBAL OCEAN WIND L4 NEAR REAL TIME 6 HOURLY OBSERVATIONS](https://resources.marine.copernicus.eu/?option=com_csw&view=details&product_id=WIND_GLO_WIND_L4_NRT_OBSERVATIONS_012_004). 

```{r wind speed}

# Open netcdf file 
wind <- nc_open("../Data_Raw/CERSAT-GLO-BLENDED_WIND_L4-V6-OBS_FULL_TIME_SERIE_1626911972037.nc")
# Save metadata to a text file
{
  sink('../Data_Raw/CERSAT-GLO-BLENDED_WIND_L4-V6-OBS_FULL_TIME_SERIE_16269119720374.txt')
  print(wind)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(wind, "lon")
lat <- ncvar_get(wind, "lat", verbose = F)
t <- ncvar_get(wind, "time")

head(lon)

# Read in data from the wind variable and verify the dimensions of the array
wind.array <- ncvar_get(wind, "wind_speed") # 3dim array
dim(wind.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(wind, "wind_speed","_FillValue")
fillvalue

wind.array[wind.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(wind)

# Isolate and plot a random time step to check
wind.slice <- wind.array[,,1]

dim(wind.slice) #2dim

wind.r <- raster(t(wind.slice), xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # Found projection on the website
                crs=CRS("+proj=longlat +datum=WGS84 +no_defs")) %>%
         flip(direction="y")%>%
         raster::projectRaster(crs=prj)

wind.df <- as.data.frame(wind.r, xy=TRUE) %>% drop_na()
colnames(wind.df) <- c("x","y","windspeed")

plot(wind.r)


# Map of study area with wind data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=wind.df, aes(x=x, y=y, fill=windspeed), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")


test <- aperm(wind.array, c(2,1,3))
# Make a raster brick of all values 
wind_brick <- brick(test, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS("+proj=longlat +datum=WGS84 +no_defs")) %>% 
         flip(direction="y") %>%
         raster::projectRaster(crs=prj) 

# Convert date from seconds since 01/01/1970 to yyyy-mm-dd format
t2 <- as.POSIXct("1900-01-01 00:00")+as.difftime(t,units="hours")
class(t2)
# Name raster layers after the date that they portray
names(wind_brick) <-t2

# Save output file
# writeRaster(wind_brick, "../Data_Processed/wind_AOOS_cropped_4336.tif")

# Fun animation of the raster brick
# animate(wind_brick, pause=0.5, n=1)

# Calculate mean monthly wind rasters 

#get the date from the names of the layers and extract the month
indices <- format(as.Date(names(wind_brick), format = "X%Y.%m.%d"), format = "%Y%m")
indices <- as.numeric(indices)

#sum layers
wind_month <- stackApply(wind_brick, indices, fun = mean)
# animate(wind_month, pause=0.5, n=1)

# plot monthly average wind data for November, 2018
windmonth.df <- as.data.frame(wind_month[[1]], xy=TRUE)  %>% drop_na()
colnames(windmonth.df) <- c("x","y","windspeed")

ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=windmonth.df, aes(x=x, y=y, fill=windspeed), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

# animate(wind_month, pause=0.5, n=1)

# Save output file
# writeRaster(wind_month, "../Data_Processed/WIND_Monthly.nc",
              # overwrite=TRUE, format="CDF",
              # varname="wind_speed", varunit="m/s",
              # longname="Wind Speed -- raster brick to netCDF",
              # xname="lon",   yname="lat",zname="time",
              # zunit="numeric")
```

Test to make sure that the netcdf file saved correctly. 

```{r testing netcdf file save, eval=FALSE}

test <-nc_open("../Data_Processed/WIND_Monthly_cropped_4336.nc")

# Save metadata to a text file
{
  sink('../Data_Processed/WIND_Monthly_cropped_4336.txt')
  print(test)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(test, "lon")
lat <- ncvar_get(test, "lat", verbose = F)
t <- ncvar_get(test, "time")

head(lon)

# Read in data from the wind variable and verify the dimensions of the array
test.array <- ncvar_get(test, "wind_speed") # 3dim array
dim(test.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(test, "wind_speed","_FillValue")
fillvalue

test.array[test.array == fillvalue$value] <- NA

nc_close(test)

test_brick <- brick(test.array, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS(prj)) 

plot(test_brick[[1]])


test.df <- as.data.frame(test_brick[[1]], xy=TRUE)
colnames(test.df) <- c("x","y","windspeed")

plot(test.r)


# Map of study area with wind data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=test.df, aes(x=x, y=y, fill=windspeed), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")


```

## Sea Surface Temperature

[OSTIA: Operational Sea Surface Temperature and Sea Ice Analysis](https://portal.aoos.org/gulf-of-alaska.php#module-metadata/e873784f-7201-416f-bb26-6061c82af388/adc56dd8-e6c6-4d09-abc6-86962fc308eb) from the Alaska Ocean Observing System. 

Citation: 	UK Met Office. 2005. OSTIA L4 SST Analysis. Ver. 1.0. PO.DAAC, CA, USA. Dataset accessed [YYYY-MM-DD] at https://doi.org/10.5067/GHOST-4FK01

Data requested from: https://thredds.axiomdatascience.com/thredds/ncss/AOOS_OSTIA.nc/dataset.html
Variables = analysed_sst, analysis_error, mask, sea_ice_fraction

North = 62
South = 56
West = -155
East = -143
Start = 01 Nov 2018
End = 31 July 2020

NCSS Request URL: https://thredds.axiomdatascience.com/thredds/ncss/AOOS_OSTIA.nc?var=analysed_sst&var=analysis_error&var=mask&var=sea_ice_fraction&north=62&west=-155&east=-143&south=56&disableProjSubset=on&horizStride=1&time_start=2018-11-01T12%3A00%3A00Z&time_end=2020-07-31T12%3A00%3A00Z&timeStride=1&accept=netcdf

```{r sea surface temperature}

# Open netcdf file 
sst <- nc_open("../Data_Raw/AOOS_OSTIA.nc")
# Save metadata to a text file
{
  sink('../Data_Raw/AOOS_OSTIA.txt')
  print(sst)
  sink()
}

# Read lat lon and time for each observation
lon <- ncvar_get(sst, "lon")
lat <- ncvar_get(sst, "lat", verbose = F)
t <- ncvar_get(sst, "time")

head(lon)

# Read in data from the SST variable and verify the dimensions of the array
sst.array <- ncvar_get(sst, "analysed_sst") # 3dim array
dim(sst.array)

# Identify fill value and replace with NA
fillvalue <- ncatt_get(sst, "analysed_sst","_FillValue")
fillvalue

sst.array[sst.array == fillvalue$value] <- NA

# Close netcdf file
nc_close(sst)

# Isolate and plot a random time step to check
sst.slice <- sst.array[,,1]

dim(sst.slice) #2dim

sst.r <- raster(t(sst.slice), xmn=min(lon), xmx=max(lon), ymn=min(lat),ymx=max(lat),
                # Found projection on the website
                crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs ")) %>%
         flip(direction="y") %>%
         raster::projectRaster(crs=prj) %>%
         raster::crop(study)

sst.df <- as.data.frame(sst.r, xy=TRUE) %>% drop_na()
colnames(sst.df) <- c("x","y","sst")

plot(sst.r)

# Map of study area with sst data
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_raster(data=sst.df, aes(x=x, y=y, fill=sst), alpha=0.9) +
  scale_fill_gradient2() +
  geom_sf(data=study, fill=NA, color="red")

# Make a raster brick of all values 
sst_brick <- brick(sst.array, xmn=min(lon), xmx=max(lon), ymn=min(lat), ymx=max(lat), 
                crs=CRS("+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")) %>% 
         flip(direction="x") %>%
         raster::projectRaster(crs=prj) %>% 
         raster::crop(study)

# Convert date from seconds since 01/01/1970 to yyyy-mm-dd format
t <- anydate(t)
class(t)
# Name raster layers after the date that they portray
names(sst_brick) <- format(t, "X%Y.%m.%d")

# Save output file
# writeRaster(sst_brick, "../Data_Processed/SST_AOOS_cropped_4336.tif")

# Fun animation of the raster brick
# animate(sst_brick, pause=0.5, n=1)

# Calculate the change between any two layers
# tempchange <- sst_brick[[2]]-sst_brick[[1]]
# tempchange

# Calculate mean monthly SST rasters 

#get the date from the names of the layers and extract the month
indices <- format(as.Date(names(sst_brick), format = "X%Y.%m.%d"), format = "%Y%m")
indices <- as.numeric(indices)

#sum layers
sst_month <- stackApply(sst_brick, indices, fun = mean)
# animate(sst_month, pause=0.5, n=1)

# writeRaster(sst_month, "../Data_Processed/SST_Monthly.nc",
#       overwrite=TRUE, format="CDF",
#       varname="analysed_sst", varunit="m/s",
#       longname="Sea Surface Temperature -- raster brick to netCDF",
#       xname="lon",   yname="lat",zname="time",
#       zunit="numeric")

```

I used this tutorial on indexing [raster time series](https://cyberhelp.sesync.org/raster-time-series-alaska-lesson/index.html#/slides/brick)

For calculating monthly mean values in a [raster brick](https://gis.stackexchange.com/questions/237272/mean-by-month-on-r-stacked-raster). 

```{r merging sea lion locs with SST data, warnings=F, messages=F}

# Put seali date into same format as raster brick names
sealis$yearmon <- format(sealis$Date, format="%Y%m")

# Extract monthly average values for SST at seali locs
# Want to look into using an apply function to hopefully make this go faster (it takes ~5 min right now)
for(i in 1:length(sealis$DeployID)){
  if(i %% 100 == 0){
    print(i)
  }
   sealis$SST[i] <- extract(subset(sst_month, 
                                   paste0("index_",sealis$yearmon[i])), sealis[i,])
}

# Looking at the data 
hist(sealis$SST)
length(which(is.na(sealis$SST)))
length(unique(sealis$SST))
length(sealis$SST)
tail(sealis$SST)

hist(as.numeric(sealis$yearmon))
unique(sealis$yearmon)
unique(sealis$DeployID)

# # SST over time at a particular location 
# idx <- match('Aug.13.2019', names(sst_brick))
# plot(sst_brick[[idx]])

```


```{r extra fun stuff}
# Correlation between dive temperature and ste
sealitemps <- st_read("../Data_Raw/locs_depths_temps_geodbase_20201130.gdb",
                    layer=seali$name[10]) %>% st_transform(prj)

# Extract monghtly average values for SST at seali locs
# Want to look into using an apply function to hopefullymake this go faster (it takes ~5 min right now)
for(i in 1:length(sealitemps$DeployID)){
  if(i %% 100 == 0){
    print(i)
  }
   sealitemps$SST[i] <- extract(subset(sst_month, 
                                   paste0("index_",sealitemps$yearmon[i])), sealis[i,])
}


```

