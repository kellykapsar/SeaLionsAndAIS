---
title: "SSLDataProcessing"
author: "Kelly Kapsar"
date: "8/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import libraries. 
```{r message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(sf)
library(raster)
library(ggplot2)
library(scales)
library(ggmap)
library(leaflet)

```

Study area boundaries. 

```{r study area warning=FALSE}
# Projection information for WGS84/UTM Zone 5N (EPSG:32605)
prj <- 32605

# Create study area polygon
coords <- data.frame(lat=c(56, 62, 62, 56, 56), lon=c(-155, -155, -143, -143, -155), id="study")
study <- coords %>% 
         st_as_sf(coords = c("lon", "lat"), crs=4326) %>% 
         group_by(id) %>% 
         summarize(geometry = st_combine(geometry)) %>%  
         st_cast("POLYGON") %>% 
         st_transform(prj)
# st_write(study, "../Data_Raw/studyarea.shp")

basemap <- read_sf("../Data_Raw/BBmap.shp") %>% st_transform(prj) %>%  st_buffer(0)

# Crop basemap to buffered extent of study area 
study.buff <- st_buffer(study, 100000) # Buffer study area by 100 km
basemap.crop <- st_crop(basemap, study.buff)

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")

# Get latlong coordinates for study area for use in downloading other data sets
studylatlon <- study %>% st_transform(4269) %>% st_bbox()

```

## Sea Lion location geodatabase

```{r sea lion gdb processing}
# Import sea lion location geodatabase (downloaded from Google Drive folder)
# seali <- st_layers("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb")
seali <- list.files("../Data_Raw/raw data files - complete - all tags-20210803T143355Z-001")
# seali$name # list of layers 

# Determine all layers of interest within gdb
# lyrs <- grep("_loc", seali$name)
lyrs <- grep("S-Locations", seali)
lyrs2 <- grep("D-Locations", seali)

lyrs <- append(lyrs, lyrs2)

# Create initial sf object with data from one sea lion
lyrs <- lapply(lyrs, function(x){st_read(paste0("../Data_Raw/raw data files - complete - all tags-20210803T143355Z-001/",seali[x]))})

# Make each table into an sf object
# lyrs <- lapply(lyrs, function(x){st_as_sf(x, coords = c("Longitude","Latitude"), crs=4326)})

# Separate first table layer
sealis <- lyrs[[1]]

# Remove that layer from the layers of interest list
lyrs <- lyrs[2:length(lyrs)]
# Append all other layers of interest onto the main location data set 
for(i in 1:length(lyrs)){
  temp <- lyrs[[i]]
  sealis <- rbind(sealis, temp)
}

# Fix time field 
# (Have to do it separately for gps and argos)
sealis$Date_old <- sealis$Date
gps <- sealis[sealis$Type == "FastGPS",]
argos <- sealis[sealis$Type == "Argos",]


argos$Date <- as.POSIXct(argos$Date, format=c("%Y/%m/%d %H:%M:%S"), tz="GMT")
# All but 353 gps points are in the format of days since 12/30/1899
# Need to convert those to Dates and then also fix the other 300ish points 
# Internet said it should be days since 1/1/1900, but that didn't work. No idea why. 
# But this matches up with the Microsoft Access database
gps$Date <-   as.POSIXct("1899-12-30 00:00:00", tz="GMT")+ 
  as.difftime(as.numeric(gps$Date),units="days")
gps$Date[is.na(gps$Date)] <- as.POSIXct(gps$Date_old[is.na(gps$Date)], format=c("%Y/%m/%d %H:%M:%S"),
                                        tz="GMT")

sealis_old <- sealis 

# Rejoin Argos and GPS data 
sealis <- rbind(gps, argos)

# Round date to minute scale 
sealis$Date <- round(sealis$Date, "mins")


# Create various date reference categories for future modeling 
sealis$fortnight <- ceiling(lubridate::week(sealis$Date) / 2)
sealis$weekofyear <- lubridate::week(sealis$Date)
sealis$month <- lubridate::month(sealis$Date)
sealis$year <- lubridate::year(sealis$Date)
sealis$dayofyear <- lubridate::date(sealis$Date)

# Remove low quality points 
sealis <- sealis[-which(sealis$Quality %in% c("A","B","0","Z")),]
# sealis$inbounds <- lengths(st_within(sealis, st_transform(study, 4326)))
# sealis <- sealis[which(sealis$inbounds == TRUE),]

############ CHECK ON THIS ###############
# 40 Duplicated rows (excluding geometry column)
test <- duplicated(data.frame(sealis))
# Remove duplicated rows 
sealis <- sealis[which(duplicated(data.frame(sealis))==FALSE),]

# Remove points from same time and SSL (keep smaller error radius)
sealis$ptID <- 1:length(sealis$DeployID)
test <- sealis %>% filter(Type == "Argos") %>% group_by(DeployID, Date) %>% slice(which.min(Error.radius))
test2 <- filter(sealis, Type != "Argos")
sealis <- rbind(test, test2)

# Convert to spatial object
sealis <- st_as_sf(sealis, coords=c("Longitude", "Latitude"), crs=4326)

# Keep latitude and longitude columns
sealis$lat <- st_coordinates(sealis)[,"Y"]
sealis$lon <- st_coordinates(sealis)[,"X"]

# Transform geometry to correct projection 
st_geometry(sealis) <- st_transform(st_geometry(sealis), prj)

# Projected coordinate columns
sealis$northing <- st_coordinates(sealis)[,"Y"]
sealis$easting <- st_coordinates(sealis)[,"X"]

# Remove blank columns 
sealis <- sealis[,-c(11:15)]

# Save clean seali data 
sealis <- rename(sealis, ErrorRad = Error.radius, ErrorMajor = Error.Semi.major.axis, 
               ErrorMinor = Error.Semi.minor.axis, ErrorEllipse = Error.Ellipse.orientation)

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")+
  geom_sf(data=sealis, aes(color=DeployID))

# Tag duration timeline 
timeline <- sealis %>% st_drop_geometry() %>% group_by(DeployID) %>% summarize(starttag = as.Date(min(Date)), endtag=as.Date(max(Date)))

ggplot(timeline, aes(x=starttag, y= DeployID)) +
  geom_linerange(aes(xmin = starttag, xmax = endtag),color = "black",size = 2) + 
  scale_x_date(breaks=date_breaks(width="1 month"), date_labels="%b %Y") +
  theme(axis.text.x=element_text(angle=45, hjust=1))+
  ylab("") +
  xlab("") 


``` 


```{r available location -- radius method}

# Calculating a (average hrly movement rate) for each SSL (km/hr) and b (sd of movement rate)
euclidean_speed <- function(lat2, lat1, long2, long1, time2, time1) {
  latdiff <- lat2 - lat1
  longdiff <- long2 - long1
  distance <- sqrt(latdiff^2 + longdiff^2)/1000
  timediff <- as.numeric(difftime(time2,time1,units=c("hours")))
  return(distance / timediff)
}
  

# Calculate Euclidean speed
sealis <- sealis %>% 
  group_by(DeployID) %>%
  arrange(DeployID, Date) %>% 
  mutate(difftime = timediff <- as.numeric(difftime(Date,lag(Date),units=c("mins"))),
         speed = euclidean_speed(northing, lag(northing), easting, lag(easting), Date, lag(Date)))

# Identify locations with another signal occurring within the same minute 
sameminute <- sealis %>% st_drop_geometry() %>% group_by(DeployID, Date) %>% tally() %>% filter(n > 1)

rm <- list()

for(i in 1:length(sameminute$DeployID)){
  t <- sealis[which(sealis$DeployID == sameminute$DeployID[i] & sealis$Date == sameminute$Date[i]),]
  if(length(unique(t$Type)) > 1){
  for(j in 1:length(t$DeployID)){
    if(t$Type[j] != "FastGPS"){ rm <- append(rm, t$ptID[j])}
  }
  }
  if(length(unique(t$Type)) == 1){rm <- append(rm, t$ptID[1])}
}

# Remove observations from the data set 
sealis <- sealis[-which(sealis$ptID %in% rm),]
sealis <- sealis[-which(sealis$ptID == 48742),] # manually remove last sameminute point 

# Recalculate Euclidean speed
sealis <- sealis %>% 
  group_by(DeployID) %>%
  arrange(DeployID, Date) %>% 
  mutate(timediff = as.numeric(difftime(Date,lag(Date),units=c("mins"))),
         speed = euclidean_speed(northing, lag(northing), easting, lag(easting), 
                                 Date, lag(Date)))

# Remove points over 75 km/hr as unreasonable speeds
sealis <- sealis %>% filter(speed < 50)

# Need to clean out inf and NA speed as well as those above a certain threhold
# How to determine threshold?
sealispeed <- sealis %>% 
  st_drop_geometry() %>% 
  group_by(DeployID) %>% 
  summarize(speed_avg = mean(speed, na.rm=T), 
            speed_sd = sd(speed, na.rm=T), 
            timediff = mean(timediff, na.rm=T)/60) # convert to hourly

# radius = c(a + 2b)
sealispeed$radius <- sealispeed$timediff*(sealispeed$speed_avg + 2*sealispeed$speed_sd) 

sealis <- left_join(sealis, sealispeed, by="DeployID")

```

```{r average number of non-land points per sea lion per time period}
# Read in landmask 
landmask <- raster("../Data_Processed/Landmask_GEBCO.tif")

sealis$land <- raster::extract(landmask, sealis)
sum(sealis$land, na.rm=T)/length(sealis$land)*100

watersealis <- sealis[is.na(sealis$land),]

ptcts_month <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, year, month) %>% summarize(n=n())
ptcts_month <- ptcts_month%>% group_by(DeployID) %>% summarize(meanmonthlypts = mean(n))

ptcts_biweek <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, year, fortnight) %>% summarize(n=n())
ptcts_biweek <- ptcts_biweek%>% group_by(DeployID) %>% summarize(meanbiweekpts = mean(n))

ptcts_week <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, year, weekofyear) %>% summarize(n=n())
ptcts_week <- ptcts_week%>% group_by(DeployID) %>% summarize(meanweekpts = mean(n))

ptcts_day <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, dayofyear) %>% summarize(n=n())
ptcts_day <- ptcts_day%>% group_by(DeployID) %>% summarize(meandaypts = mean(n))

ptcts <- left_join(ptcts_month, ptcts_biweek, by="DeployID")
ptcts <- left_join(ptcts, ptcts_week, by="DeployID")
ptcts <- left_join(ptcts, ptcts_day, by="DeployID")

# write.csv(ptcts, "../Data_Raw/SSL_PtCts.csv")

# watersealidata <- data.frame(stat=c(), monthly=c(), biweekly=c(), weekly=c(), daily=c())
# watersealidata[1,"stat"] <- "mean"
# watersealidata[1,c("monthly", "biweekly", "weekly", "daily")] <- unlist(lapply(ptcts[,2:5], mean))
# 
# watersealidata[2,"stat"] <- "stdev"
# watersealidata[2,c("monthly", "biweekly", "weekly", "daily")] <- unlist(lapply(ptcts[,2:5], sd))

```

```{r create buffers around used locations}
# Create individually buffered points based on 
sealibuffs <- watersealis %>% group_by(DeployID) %>% st_buffer(dist=sealis$radius*1000) # Convert radius to m

saveRDS(sealibuffs, "../Data_Processed/watersealibuffs.rds")
saveRDS(watersealis, "../Data_Processed/watersealis.rds")
```

```{r}

# Read in covariates 
landmask <- raster("../Data_Processed/Landmask_GEBCO.tif")
dist500m <- raster("../Data_Processed/Dist500m.tif") %>% raster::mask(landmask, maskvalue=1)
depth <- raster("../Data_Processed/Bathymetry.tif") %>% raster::mask(landmask, maskvalue=1)
distland <- raster("../Data_Processed/DistLand.tif") %>% raster::mask(landmask, maskvalue=1)
covarstack <- raster::stack(distland, dist500m, depth)

ship <- readRDS("../Data_Processed/AIS_AllOther.rds")
fish <- readRDS("../Data_Processed/AIS_Fishing.rds")
eke1 <- readRDS("../Data_Processed/eke_weekly_20181101-20200531.rds")
eke2 <- readRDS("../Data_Processed/eke_weekly_20200601-20200731.rds")
ssh1 <- readRDS("../Data_Processed/ssh_weekly_20181101-20200531.rds")
ssh2 <- readRDS("../Data_Processed/ssh_weekly_20200601-20200731.rds")
sst <- readRDS("../Data_Processed/sst_weekly.rds")
wind <- readRDS("../Data_Processed/wind_weekly.rds")




used <- readRDS("../Data_Processed/watersealis.rds")
usedbuff <- readRDS("../Data_Processed/watersealibuffs.rds")

# used$Date <- format(used$Date, "%G-W%V")

# Convert used locations to trk objects using amt package
library(amt)

trk <- mk_track(st_drop_geometry(used), .x=lon, .y=lat, .t=Date, id = DeployID, 
                crs = CRS("+init=epsg:4326"))
trk.class<-class(trk)

# Calculate time of day based on lat/lon and timestamp
trk <- trk %>% time_of_day()
class(trk) <- trk.class

#' Now, we can transform back to geographic coordinates
trk <- amt::transform_coords(trk, CRS("+init=epsg:32605"))

# Extract covariate data at used locations 
test <- extract_covariates_var_time(trk, sst, when="any", max_time= weeks(1), name_covar="SST")



f <- extract_covariates_var_time(trk, fish, when="any", max_time= days(16), name_covar="Fishing")
s <- extract_covariates_var_time(trk, ship, when="any", max_time= days(16), name_covar="Shipping")




############################### IDENTIFYING AVAILABLE POINTS IN OCEAN 

# ID available points based on previously calculated radii
# Accounts for points on land 
custom_extract_avail <- function(usedbuff, covarstack, npts = 5){
  df <-  as.data.frame(sampleRandom(x=covarstack, size = npts, na.rm = TRUE, ext = as(usedbuff, "Spatial"), xy = TRUE))
  df$Used <- 0
  df$Date <- usedbuff$Date
  df$DeployID <- usedbuff$DeployID
  return(df)
}

# Runing function as a loop takes ~15 hrs.... 
start <- proc.time()
q <- lapply(1:length(usedbuff$DeployID), 
            function(x){custom_extract_avail(usedbuff[x,], covarstack=covarstack)})
avail <- do.call(rbind, q)
proc.time()-start

writeRDS(avail, "../Data_Processed/AvailableLocsWithCovars.rds")


# Doesn't account for points on land 
q <- st_sample(usedbuff[1:10,], size = rep(5, length(usedbuff$DeployID[1:10])), by_polygon=TRUE)

# Doesn't account for points on land
start <- proc.time()
q <- lapply(1:length(usedbuff$DeployID), 
            function(x){random_points(usedbuff[x,], n=5)})
avail <- do.call(rbind, q)
proc.time()-start


########################################################################################################
#   WORKING ON SPATIOTEMPORAL COVAR EXTRACTION

test <- extract_covariates_var_time(trk, sst, when="any", max_time= weeks(1), name_covar="SST")


### COPIED FROM AMT PACKAGE
# https://github.com/jmsigner/amt/blob/master/R/extract_covariates.R
# Also helpful: https://cran.r-project.org/web/packages/amt/vignettes/p3_rsf.html

t <- trk$t_

xy <- trk[,c("x_","y_")]

covariates <- sst
when <- "any"
max_diff <- lubridate::period(weeks=1)

extract_covar_var_time_base <- function(
  xy, t, covariates, when = "any",
  max_diff) {

  if (is.null(raster::getZ(covariates))) {
    stop("Covariates do not have a Z column.")
  }

  if (!is(max_diff, "Period")) {
    stop("`max_diff` is not of class `Period`.")
  }
  max_diff <- lubridate::period_to_seconds(max_diff)
  t_covar <- as.numeric(as.POSIXct(raster::getZ(covariates), format="%G-W%V"))
  t_obs <- as.numeric(as.POSIXct(t))

  # Fun to find closest point
  which_rast <- function(t_diffs, where, max_diff) {
    wr <- if (when == "after") {
      which.min(t_diffs[t_diffs >= 0])
    } else if (when == "before") {
      which.min(abs(t_diffs[t_diffs <= 0])) + sum(t_diffs > 0)
    } else if (when == "any") {
      which.min(abs(t_diffs))
    }
    if (length(wr) == 0) {
      NA
    } else if (max_diff < abs(t_diffs[wr])) {
      NA
    } else {
      wr
    }
  }

  wr <- sapply(t_obs, function(x) which_rast(x - t_covar, when, max_diff))
  ev <- raster::extract(covariates, cbind(xy))
  cov_val <- ev[cbind(seq_along(wr), wr)]
  return(cov_val)
}



t_covar <- as.numeric(as.POSIXct(raster::getZ(fish)))
t_obs <- as.numeric(as.POSIXct(t))
```


```{r plot individual SSL locs}

#' ### Using ggplot without a background
#' 
#' Use separate axes for each individual (add scales="free" to facet_wrap)
#+fig.height=12, fig.width=12
ggplot(sealis, aes(x=lon, y=lat))+geom_point()+
  facet_wrap(~DeployID, scales="free")


# test individual ssls 
ssl781 <- sealis %>% filter(DeployID == "SSL2019781KOD")

# Leaflet map 
leaflet(ssl781)%>%addTiles()%>%
  addCircles(~lon, ~lat)

# ggmap 
map <- get_map(location = c(lon = mean(ssl781$lon), 
                            lat = mean(ssl781$lat)), zoom = 7,
               maptype = "hybrid", source = "google")

ggmap(map) + 
  geom_point(data=ssl781, aes(x=lon, y=lat), size=2.5)

############################# TESTING AMT PACKAGE ##################################
library(amt)
library(purrr)

sealis$DeployID <- as.factor(sealis$DeployID)

trk <- mk_track(st_drop_geometry(sealis), .x=lon, .y=lat, .t=Date, id = DeployID, 
                crs = CRS("+init=epsg:4326"))
trk.class<-class(trk)

# Calculate time of day based on lat/lon and timestamp
trk <- trk %>% time_of_day()
class(trk) <- trk.class

#' Now, we can transform back to geographic coordinates
trk <- amt::transform_coords(trk, CRS("+init=epsg:32605"))

#' Or, we can add a columns to each nested column of data using purrr::map
trk <- trk %>% nest_legacy(-id) %>% 
  mutate(dir_abs = map(data, direction_abs,full_circle=TRUE, zero="N"), 
         dir_rel = map(data, direction_rel), 
         sl = map(data, step_lengths),
         nsd_=map(data, nsd))%>%unnest()


```
