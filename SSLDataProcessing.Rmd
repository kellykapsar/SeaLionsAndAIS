---
title: "SSLDataProcessing"
author: "Kelly Kapsar"
date: "8/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Import libraries. 
```{r message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(sf)
library(raster)
library(ggplot2)
library(scales)
library(ggmap)
library(leaflet)

```

Study area boundaries. 

```{r study area warning=FALSE}
# Projection information for WGS84/UTM Zone 5N (EPSG:32605)
prj <- 32605

# Create study area polygon
coords <- data.frame(lat=c(56, 62, 62, 56, 56), lon=c(-155, -155, -143, -143, -155), id="study")
study <- coords %>% 
         st_as_sf(coords = c("lon", "lat"), crs=4326) %>% 
         group_by(id) %>% 
         summarize(geometry = st_combine(geometry)) %>%  
         st_cast("POLYGON") %>% 
         st_transform(prj)
# st_write(study, "../Data_Raw/studyarea.shp")

basemap <- read_sf("../Data_Raw/BBmap.shp") %>% st_transform(prj) %>%  st_buffer(0)

# Crop basemap to buffered extent of study area 
study.buff <- st_buffer(study, 100000) # Buffer study area by 100 km
basemap.crop <- st_crop(basemap, study.buff)

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")

# Get latlong coordinates for study area for use in downloading other data sets
studylatlon <- study %>% st_transform(4269) %>% st_bbox()

```

## Sea Lion location geodatabase

```{r sea lion gdb processing}
# Import sea lion location geodatabase (downloaded from Google Drive folder)
# seali <- st_layers("../Data_Raw/SSL Adult Female Analysis 2018-20.gdb")
seali <- list.files("../Data_Raw/raw data files - complete - all tags-20210803T143355Z-001")
# seali$name # list of layers 

# Determine all layers of interest within gdb
# lyrs <- grep("_loc", seali$name)
lyrs <- grep("S-Locations", seali)
lyrs2 <- grep("D-Locations", seali)

lyrs <- append(lyrs, lyrs2)

# Create initial sf object with data from one sea lion
lyrs <- lapply(lyrs, function(x){st_read(paste0("../Data_Raw/raw data files - complete - all tags-20210803T143355Z-001/",seali[x]))})

# Make each table into an sf object
# lyrs <- lapply(lyrs, function(x){st_as_sf(x, coords = c("Longitude","Latitude"), crs=4326)})

# Separate first table layer
sealis <- lyrs[[1]]

# Remove that layer from the layers of interest list
lyrs <- lyrs[2:length(lyrs)]
# Append all other layers of interest onto the main location data set 
for(i in 1:length(lyrs)){
  temp <- lyrs[[i]]
  sealis <- rbind(sealis, temp)
}

# Fix time field 
# (Have to do it separately for gps and argos)
sealis$Date_old <- sealis$Date
gps <- sealis[sealis$Type == "FastGPS",]
argos <- sealis[sealis$Type == "Argos",]


argos$Date <- as.POSIXct(argos$Date, format=c("%Y/%m/%d %H:%M:%S"), tz="GMT")
# All but 353 gps points are in the format of days since 12/30/1899
# Need to convert those to Dates and then also fix the other 300ish points 
# Internet said it should be days since 1/1/1900, but that didn't work. No idea why. 
# But this matches up with the Microsoft Access database
gps$Date <-   as.POSIXct("1899-12-30 00:00:00", tz="GMT")+ 
  as.difftime(as.numeric(gps$Date),units="days")
gps$Date[is.na(gps$Date)] <- as.POSIXct(gps$Date_old[is.na(gps$Date)], format=c("%Y/%m/%d %H:%M:%S"),
                                        tz="GMT")

sealis_old <- sealis 

# Rejoin Argos and GPS data 
sealis <- rbind(gps, argos)

# Round date to minute scale 
sealis$Date <- round(sealis$Date, "mins")


# Create various date reference categories for future modeling 
sealis$fortnight <- ceiling(lubridate::week(sealis$Date) / 2)
sealis$weekofyear <- lubridate::isoweek(sealis$Date)
sealis$month <- lubridate::month(sealis$Date)
sealis$year <- lubridate::year(sealis$Date)
sealis$dayofyear <- lubridate::date(sealis$Date)

# Remove low quality points 
sealis <- sealis[-which(sealis$Quality %in% c("A","B","0","Z")),]
# sealis$inbounds <- lengths(st_within(sealis, st_transform(study, 4326)))
# sealis <- sealis[which(sealis$inbounds == TRUE),]

############ CHECK ON THIS ###############
# 40 Duplicated rows (excluding geometry column)
test <- duplicated(data.frame(sealis))
# Remove duplicated rows 
sealis <- sealis[which(duplicated(data.frame(sealis))==FALSE),]

# Remove points from same time and SSL (keep smaller error radius)
sealis$ptID <- 1:length(sealis$DeployID)
test <- sealis %>% filter(Type == "Argos") %>% group_by(DeployID, Date) %>% slice(which.min(Error.radius))
test2 <- filter(sealis, Type != "Argos")
sealis <- rbind(test, test2)

# Put back in temporal order by sea lion
sealis <- sealis[order(sealis$DeployID, sealis$Date),]

# Convert to spatial object
sealis <- st_as_sf(sealis, coords=c("Longitude", "Latitude"), crs=4326)

# Keep latitude and longitude columns
sealis$lat <- st_coordinates(sealis)[,"Y"]
sealis$lon <- st_coordinates(sealis)[,"X"]

# Transform geometry to correct projection 
st_geometry(sealis) <- st_transform(st_geometry(sealis), prj)

# Projected coordinate columns
sealis$northing <- st_coordinates(sealis)[,"Y"]
sealis$easting <- st_coordinates(sealis)[,"X"]

# Remove blank columns 
sealis <- subset(sealis, select=-c(Offset, Offset.orientation, GPE.MSD, GPE.U, Count))

# Save clean seali data 
sealis <- rename(sealis, ErrorRad = Error.radius, ErrorMajor = Error.Semi.major.axis, 
               ErrorMinor = Error.Semi.minor.axis, ErrorEllipse = Error.Ellipse.orientation)

# Implement speed filter 
sealis <- sealis %>% arrange(DeployID, Date) %>% 
                group_by(DeployID) %>% 
                mutate(spdfilt = argosfilter::vmask(lat=lat, lon=lon, dtime=Date, vmax = 3)) %>% 
                ungroup()

# Remove speed filtered points 
sealis_dirty <- sealis 
sealis <- sealis %>%  dplyr::filter(spdfilt != "removed") %>% dplyr::arrange(Date)

# Plot change between speed filtered and original, separated by ssl
sealis_dirty %>% st_drop_geometry() %>% 
            dplyr::arrange(Date) %>% 
            ggplot() + 
              geom_path(aes(x=lon, y=lat)) + 
              geom_path(data=sealis, aes(x=lon, y=lat), col="red") + 
              facet_wrap(~DeployID, scales = "free")

# plot clean data by ssl
# Plot change between speed filtered and original, separated by ssl
sealis %>% st_drop_geometry() %>% 
            dplyr::arrange(Date) %>% 
            ggplot() + 
              geom_path(aes(x=lon, y=lat)) + 
              facet_wrap(~DeployID, scales = "free")

# Map of study area 
ggplot() +
  geom_sf(data=basemap.crop, fill="gray", color="black", lwd=0.5) +
  geom_sf(data=study, fill=NA, color="red")+
  # geom_path is much faster than geom_sf
  geom_path(data=sealis, aes(x=easting, y=northing, color=DeployID))

# Tag duration timeline 
timeline <- sealis %>% st_drop_geometry() %>% group_by(DeployID) %>% summarize(starttag = as.Date(min(Date)), endtag=as.Date(max(Date)))

ggplot(timeline, aes(x=starttag, y= DeployID)) +
  geom_linerange(aes(xmin = starttag, xmax = endtag),color = "black",size = 2) + 
  scale_x_date(breaks=date_breaks(width="1 month"), date_labels="%b %Y") +
  theme(axis.text.x=element_text(angle=45, hjust=1))+
  ylab("") +
  xlab("") 


``` 


```{r available location -- radius method}

# Calculating a (average hrly movement rate) for each SSL (km/hr) and b (sd of movement rate)
euclidean_speed <- function(lat2, lat1, long2, long1, time2, time1) {
  latdiff <- lat2 - lat1
  longdiff <- long2 - long1
  distance <- sqrt(latdiff^2 + longdiff^2)/1000
  timediff <- as.numeric(difftime(time2,time1,units=c("hours")))
  return(distance / timediff)
}
  

# # Calculate Euclidean speed
# sealis <- sealis %>% 
#   group_by(DeployID) %>%
#   arrange(DeployID, Date) %>% 
#   mutate(difftime = timediff <- as.numeric(difftime(Date,lag(Date),units=c("mins"))),
#          speed = euclidean_speed(northing, lag(northing), easting, lag(easting), Date, lag(Date)))
# 
# # Identify locations with another signal occurring within the same minute 
# sameminute <- sealis %>% st_drop_geometry() %>% group_by(DeployID, Date) %>% tally() %>% filter(n > 1)
# 
# rm <- list()
# 
# for(i in 1:length(sameminute$DeployID)){
#   t <- sealis[which(sealis$DeployID == sameminute$DeployID[i] & sealis$Date == sameminute$Date[i]),]
#   if(length(unique(t$Type)) > 1){
#   for(j in 1:length(t$DeployID)){
#     if(t$Type[j] != "FastGPS"){ rm <- append(rm, t$ptID[j])}
#   }
#   }
#   if(length(unique(t$Type)) == 1){rm <- append(rm, t$ptID[1])}
# }
# 
# # Remove observations from the data set 
# sealis <- sealis[-which(sealis$ptID %in% rm),]
# sealis <- sealis[-which(sealis$ptID == 48742),] # manually remove last sameminute point 

# Recalculate Euclidean speed
sealis <- sealis %>% 
  group_by(DeployID) %>%
  arrange(DeployID, Date) %>% 
  mutate(timediff = as.numeric(difftime(Date,lag(Date),units=c("mins"))),
         speed_kmhr = euclidean_speed(northing, lag(northing), easting, lag(easting), 
                                 Date, lag(Date)))


# Need to clean out inf and NA speed as well as those above a certain threhold
# How to determine threshold?
sealispeed <- sealis %>% 
  st_drop_geometry() %>% 
  group_by(DeployID) %>% 
  summarize(speed_avg = mean(speed_kmhr, na.rm=T), 
            speed_sd = sd(speed_kmhr, na.rm=T), 
            timediff = mean(timediff, na.rm=T)/60) # convert to hourly

# radius = c(a + 2b)
sealispeed$radius <- sealispeed$timediff*(sealispeed$speed_avg + 2*sealispeed$speed_sd) 

sealis <- left_join(sealis, sealispeed, by="DeployID")

```

```{r average number of non-land points per sea lion per time period}
# Read in landmask 
landmask <- raster("../Data_Processed/Landmask_GEBCO.tif")

sealis$land <- raster::extract(landmask, sealis)
sum(sealis$land, na.rm=T)/length(sealis$land)*100

watersealis <- sealis[is.na(sealis$land),]

ptcts_month <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, year, month) %>% summarize(n=n())
ptcts_month <- ptcts_month%>% group_by(DeployID) %>% summarize(meanmonthlypts = mean(n))

ptcts_biweek <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, year, fortnight) %>% summarize(n=n())
ptcts_biweek <- ptcts_biweek%>% group_by(DeployID) %>% summarize(meanbiweekpts = mean(n))

ptcts_week <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, year, weekofyear) %>% summarize(n=n())
ptcts_week <- ptcts_week%>% group_by(DeployID) %>% summarize(meanweekpts = mean(n))

ptcts_day <- watersealis %>% st_drop_geometry() %>% group_by(DeployID, dayofyear) %>% summarize(n=n())
ptcts_day <- ptcts_day%>% group_by(DeployID) %>% summarize(meandaypts = mean(n))

ptcts <- left_join(ptcts_month, ptcts_biweek, by="DeployID")
ptcts <- left_join(ptcts, ptcts_week, by="DeployID")
ptcts <- left_join(ptcts, ptcts_day, by="DeployID")

# write.csv(ptcts, "../Data_Raw/SSL_PtCts.csv")

# watersealidata <- data.frame(stat=c(), monthly=c(), biweekly=c(), weekly=c(), daily=c())
# watersealidata[1,"stat"] <- "mean"
# watersealidata[1,c("monthly", "biweekly", "weekly", "daily")] <- unlist(lapply(ptcts[,2:5], mean))
# 
# watersealidata[2,"stat"] <- "stdev"
# watersealidata[2,c("monthly", "biweekly", "weekly", "daily")] <- unlist(lapply(ptcts[,2:5], sd))

```

```{r create buffers around used locations}
# Create individually buffered points based on 
sealibuffs <- watersealis %>% group_by(DeployID) %>% st_buffer(dist=sealis$radius*1000) # Convert radius to m

saveRDS(sealibuffs, "../Data_Processed/watersealibuffs.rds")
saveRDS(watersealis, "../Data_Processed/watersealis.rds")
```

```{r}

# Read in covariates 
landmask <- raster("../Data_Processed/Landmask_GEBCO.tif")
dist500m <- raster("../Data_Processed/Dist500m.tif") %>% raster::mask(landmask, maskvalue=1)
depth <- raster("../Data_Processed/Bathymetry.tif") %>% raster::mask(landmask, maskvalue=1)
distland <- raster("../Data_Processed/DistLand.tif") %>% raster::mask(landmask, maskvalue=1)
slope <- raster("../Data_Processed/slope.tif") %>% raster::mask(landmask, maskvalue=1)
covarstack <- raster::stack(distland, dist500m, depth, slope)

ship <- readRDS("../Data_Processed/AIS_AllOther.rds") 
fish <- readRDS("../Data_Processed/AIS_Fishing.rds")
eke1 <- readRDS("../Data_Processed/eke_weekly_20181101-20200531.rds")
eke2 <- readRDS("../Data_Processed/eke_weekly_20200601-20200731.rds")
ssh1 <- readRDS("../Data_Processed/ssh_weekly_20181101-20200531.rds")
ssh2 <- readRDS("../Data_Processed/ssh_weekly_20200601-20200731.rds")
sst <- readRDS("../Data_Processed/sst_weekly.rds")
wind <- readRDS("../Data_Processed/wind_weekly.rds")

# Check resolution of all covars
temporal <- list(depth, distland, dist500m, slope, ship, fish, sst, eke1, eke2, ssh1, ssh2, wind)
rasres <- lapply(temporal, function(x) res(x)/1000)

t <- data.frame(matrix(unlist(rasres), nrow=length(rasres), byrow=TRUE))
t <- cbind(data.frame(covar = c("depth", "distland", "dist500m", "slope", "ship", "fish", "sst", 'eke1', 
                                "eke2", 'ssh1', "ssh2", "wind")), t)

# Load in used locations 
used <- readRDS("../Data_Processed/watersealis.rds")
usedbuff <- readRDS("../Data_Processed/watersealibuffs.rds")

used$Used <- 1


# ID available points based on previously calculated radii
# Accounts for points on land 
custom_extract_avail <- function(usedbuff, covarstack, npts = 5){
  df <-  as.data.frame(sampleRandom(x=covarstack, size = npts, na.rm = TRUE, ext = as(usedbuff, "Spatial"), xy = TRUE))
  df$Used <- 0
  df$Date <- usedbuff$Date
  df$DeployID <- usedbuff$DeployID
  return(df)
}

# Extract available locations from buffered used locations 
# Takes ~ 1.3 hours 
# 
# start <- proc.time()
# q <- lapply(1:length(usedbuff$DeployID), 
#             function(x){custom_extract_avail(usedbuff[x,], covarstack=covarstack)})
# avail <- do.call(rbind, q)
# proc.time()-start
# 
# saveRDS(avail, "../Data_Processed/AvailableLocsWithCovars.rds")

# Read in previously extracted available locations (created from above code chunk)
avail <- readRDS("../Data_Processed/AvailablelocswithCovars.rds")


# Extract covariate data at used locations 
### Modified from FROM AMT PACKAGE
# https://github.com/jmsigner/amt/blob/master/R/extract_covariates.R
# Also helpful: https://cran.r-project.org/web/packages/amt/vignettes/p3_rsf.html
# I modified it so that it works with weekly timescale data 
# DOES NOT work with holes in the data set any more 

extract_covar_var_time_custom <- function(
  xy, t, covariates) {

  t_covar <- raster::getZ(covariates)
  t_obs <- format(as.POSIXct(t), "%G-W%V") # Convert timestamp to week

  wr <- sapply(t_obs, function(x) which(x == t_covar)) # Identify which slice to select
  ev <- raster::extract(covariates, xy) # Extract covariate values for all time slices at all used locations 
  cov_val <- ev[cbind(seq_along(wr), wr)] # select only the relevant time slice for each location
  return(cov_val)
}

# Extract covariate data at used locations 
used <- cbind(used, raster::extract(covarstack, used))
used$sst <- extract_covar_var_time_custom(xy= st_coordinates(used), t = used$Date, covariates=sst)
used$wind <- extract_covar_var_time_custom(xy= st_coordinates(used), t = used$Date, covariates=wind)
used$ship <- extract_covar_var_time_custom(xy= st_coordinates(used), t = used$Date, covariates=ship)
used$fish <- extract_covar_var_time_custom(xy= st_coordinates(used), t = used$Date, covariates=fish)
used1 <- used %>% filter(!Date > as.POSIXct("2020-06-01", format="%Y-%m-%d")) %>% mutate(
  ssh = extract_covar_var_time_custom(xy= st_coordinates(.), t = .data$Date, covariates=ssh1),
  eke = extract_covar_var_time_custom(xy= st_coordinates(.), t = .data$Date, covariates=eke1)
)
used2 <- used %>% filter(Date > as.POSIXct("2020-06-01", format="%Y-%m-%d")) %>% mutate(
  ssh = extract_covar_var_time_custom(xy= st_coordinates(.), t = .data$Date, covariates=ssh2),
  eke = extract_covar_var_time_custom(xy= st_coordinates(.), t = .data$Date, covariates=eke2)
)
used <- rbind(used1, used2)

# Extract covariate data at available locations 
avail <- avail %>% rename(northing = y, easting= x)
avail$sst <- extract_covar_var_time_custom(xy= cbind(avail$easting, avail$northing), t = avail$Date, covariates=sst)
avail$wind <- extract_covar_var_time_custom(xy= cbind(avail$easting, avail$northing), t = avail$Date, covariates=wind)
avail$ship <- extract_covar_var_time_custom(xy= cbind(avail$easting, avail$northing), t = avail$Date, covariates=ship)
avail$fish <- extract_covar_var_time_custom(xy= cbind(avail$easting, avail$northing), t = avail$Date, covariates=fish)
avail1 <- avail %>% filter(!Date > as.POSIXct("2020-06-01", format="%Y-%m-%d")) %>% mutate(
  ssh = extract_covar_var_time_custom(xy= cbind(.data$easting, .data$northing), t = .data$Date, covariates=ssh1),
  eke = extract_covar_var_time_custom(xy= cbind(.data$easting, .data$northing), t = .data$Date, covariates=eke1)
)
avail2 <- avail %>% filter(Date > as.POSIXct("2020-06-01", format="%Y-%m-%d")) %>% mutate(
  ssh = extract_covar_var_time_custom(xy= cbind(.data$easting, .data$northing), t = .data$Date, covariates=ssh2),
  eke = extract_covar_var_time_custom(xy= cbind(.data$easting, .data$northing), t = .data$Date, covariates=eke2)
)
avail <- rbind(avail1, avail2)

# Combine used and available data frames

ssl <- dplyr::bind_rows(st_drop_geometry(used), avail)
ssl$Used <- factor(ssl$Used, levels=c(1,0), labels=c("Used","Available"))
ssl$DeployID <- factor(ssl$DeployID)

saveRDS(ssl, "../Data_Processed/SSL_UsedAndAvail_WithCovars.rds")

```

```{r initial histograms}

p1 <- ggplot(ssl, aes(sst)) +
        geom_density(group=Used)

```



```{r amt package experiments}
# Convert used locations to trk objects using amt package
library(amt)

trk <- mk_track(st_drop_geometry(used), .x=lon, .y=lat, .t=Date, id = DeployID, sst=sst, 
                crs = CRS("+init=epsg:4326"))
trk.class<-class(trk)

# Calculate time of day based on lat/lon and timestamp
trk <- trk %>% time_of_day()
class(trk) <- trk.class

#' Now, we can transform back to geographic coordinates
trk <- amt::transform_coords(trk, CRS("+init=epsg:32605"))
```


```{r plot individual SSL locs}

#' ### Using ggplot without a background
#' 
#' Use separate axes for each individual (add scales="free" to facet_wrap)
#+fig.height=12, fig.width=12
ggplot(sealis, aes(x=lon, y=lat))+geom_point()+
  facet_wrap(~DeployID, scales="free")


# test individual ssls 
ssl781 <- sealis %>% filter(DeployID == "SSL2019781KOD")

# Leaflet map 
leaflet(ssl781)%>%addTiles()%>%
  addCircles(~lon, ~lat)

# ggmap 
map <- get_map(location = c(lon = mean(ssl781$lon), 
                            lat = mean(ssl781$lat)), zoom = 7,
               maptype = "hybrid", source = "google")

ggmap(map) + 
  geom_point(data=ssl781, aes(x=lon, y=lat), size=2.5)

############################# TESTING AMT PACKAGE ##################################
library(amt)
library(purrr)

sealis$DeployID <- as.factor(sealis$DeployID)

trk <- mk_track(st_drop_geometry(sealis), .x=lon, .y=lat, .t=Date, id = DeployID, 
                crs = CRS("+init=epsg:4326"))
trk.class<-class(trk)

# Calculate time of day based on lat/lon and timestamp
trk <- trk %>% time_of_day()
class(trk) <- trk.class

#' Now, we can transform back to geographic coordinates
trk <- amt::transform_coords(trk, CRS("+init=epsg:32605"))

#' Or, we can add a columns to each nested column of data using purrr::map
trk <- trk %>% nest_legacy(-id) %>% 
  mutate(dir_abs = map(data, direction_abs,full_circle=TRUE, zero="N"), 
         dir_rel = map(data, direction_rel), 
         sl = map(data, step_lengths),
         nsd_=map(data, nsd))%>%unnest()


```
